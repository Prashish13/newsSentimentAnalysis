{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>News Sentiment Analyzer using Bidirectional LSTM(BiLSTM) and Keras Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional, SpatialDropout1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We have a dataset named \"Train ready Dataset\" which has test and train folders. Inside both \"test\" and \"train\" folders there are \"neg\" and \"pos\" subfolder. There are 778 .txt files inside both \"neg\" and \"pos\" subfolder of \"train\" set, while there are 40 .txt files inside both \"neg\" and \"pos\" subfolder of \"test\" set. The text file are news excerpt from BBC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Loading and Cleaning the .txt files</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>load_file() function below loads the file,opens the file in read only mode and returns text of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load doc into the memory\n",
    "def load_file(filename):\n",
    "    #opening the file in read only mode\n",
    "    file = open(filename, 'r')\n",
    "    #read all text\n",
    "    text = file.read()\n",
    "    #close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>clean_file() function below converts the text into clean tokens by removing punctuations,stop words and short words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the text into clean tokens\n",
    "def clean_file(text):\n",
    "    #split the text into tokens by whitespace\n",
    "    tokens = text.split()\n",
    "    #remove punctuation from each token\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    #remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    #remove the stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    #remove the short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokensdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Example of loading and cleaning a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Water', 'firm', 'Suez', 'Argentina', 'row', 'conflict', 'Argentine', 'State', 'water', 'firm', 'Aguas', 'Argentinas', 'controlled', 'Frances', 'Suez', 'casting', 'doubt', 'firms', 'future', 'The', 'firm', 'serves', 'province', 'Buenos', 'Aires', 'wants', 'tariff', 'rise', 'fund', 'watersupply', 'improvements', 'The', 'government', 'rejected', 'rise', 'wants', 'Aguas', 'Argentinas', 'make', 'annual', 'investment', 'pesos', 'improvements', 'Planning', 'Minister', 'Julio', 'De', 'Vido', 'offered', 'State', 'help', 'free', 'Mr', 'De', 'Vido', 'said', 'Argentine', 'state', 'would', 'make', 'contribution', 'form', 'subsidy', 'He', 'said', 'contribution', 'could', 'made', 'return', 'seat', 'companys', 'board', 'He', 'added', 'government', 'discussions', 'Aguas', 'Argentinas', 'role', 'might', 'take', 'event', 'State', 'contribution', 'agreed', 'However', 'Aguas', 'Argentinas', 'told', 'Argentine', 'newspaper', 'Clarin', 'would', 'accept', 'change', 'legal', 'structure', 'practice', 'rules', 'State', 'participation', 'board', 'The', 'Planning', 'Minister', 'didnt', 'rule', 'possibility', 'cancelling', 'Aguas', 'Argentinas', 'water', 'concession', 'Yet', 'added', 'didnt', 'like', 'futurology', 'But', 'last', 'week', 'Argentine', 'Economic', 'Minister', 'Roberto', 'Lavagna', 'told', 'French', 'media', 'Paris', 'government', 'considering', 'allowing', 'increase', 'tariffs', 'possibility', 'State', 'contribution', 'Aguas', 'Argentinas', 'infrastructure', 'investments', 'Speaking', 'Buenos', 'Aires', 'Mr', 'De', 'Vido', 'later', 'denied', 'possibility', 'tariff', 'increase', 'insisted', 'annual', 'investment', 'water', 'infrastructure', 'centre', 'discussions', 'He', 'added', 'coming', 'weeks', 'future', 'Aguas', 'Argentinas', 'would', 'decided', 'Suez', 'owns', 'Aguas', 'Argentinas', 'Spains', 'Aguas', 'de', 'Barcelona', 'second', 'biggest', 'shareholder', 'Recently', 'Suez', 'lost', 'water', 'concession', 'Bolivia', 'mass', 'protests', 'city', 'El', 'Alto', 'poorest', 'country', 'citizens', 'complaining', 'unfair', 'water', 'charges', 'This', 'forced', 'government', 'cancel', 'contract', 'In', 'Argentina', 'Suezs', 'subsidiary', 'fined', 'cutting', 'supply', 'water', 'recent', 'heat', 'wave', 'allegedly', 'failing', 'keep', 'investment', 'meet', 'demand', 'water', 'maintained', 'tense', 'relationship', 'Argentine', 'government', 'During', 'last', 'financial', 'crisis', 'Argentina', 'firm', 'sued', 'state', 'alleging', 'converting', 'tariffs', 'US', 'dollars', 'pesos', 'freezing', 'devaluation', 'affected', 'company', 'made', 'difficult', 'meet', 'contractual', 'obligations', 'When', 'President', 'Nestor', 'Kirchner', 'Argentina', 'arrived', 'power', 'began', 'negotiate', 'solution', 'disagreements', 'international', 'utilities', 'operating', 'Argentina', 'But', 'rejected', 'tariff', 'increases', 'alleging', 'impoverish', 'citizens', 'He', 'also', 'asked', 'investments', 'meet', 'growing', 'demand', 'water', 'On', 'May', 'Aguas', 'Argentinas', 'government', 'signed', 'agreement', 'renegotiate', 'Buenos', 'Aires', 'waterconcession', 'contract', 'The', 'firm', 'agreed', 'invest', 'pesos', 'The', 'issue', 'attracted', 'European', 'interest', 'Last', 'week', 'Paris', 'President', 'Kirchner', 'discussed', 'problem', 'issues', 'French', 'president', 'Jacques', 'Chirac', 'The', 'Argentine', 'government', 'also', 'pressure', 'European', 'Union', 'countries', 'International', 'Monetary', 'Fund', 'IMF', 'raise', 'utilities', 'tariffs', 'utilities', 'operating', 'Argentina', 'European']\n"
     ]
    }
   ],
   "source": [
    "#Load and clean the document\n",
    "filename = \"Train ready Dataset/train/neg/93.txt\"\n",
    "text = load_file(filename)\n",
    "tokens = clean_file(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Define a Vocabulary</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>add_to_vocab() function performs the task of loading the document, cleaning the document and returning the tokens which is later added to vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc and add to vocab\n",
    "def add_to_vocab(filename, vocab):\n",
    "    #load  document\n",
    "    doc = load_file(filename)\n",
    "    #clean document\n",
    "    tokens = clean_file(doc)\n",
    "    #add tokens to the vocabulary\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>process_documents() function will load all the files from the directory and pass the file to add_to_vocab() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all docs in a vocabulary\n",
    "def process_documents(directory, vocab):\n",
    "    #walk through all files in the folder\n",
    "    for filename in os.listdir(directory):\n",
    "        #create a full path of the file to open\n",
    "        file_path = directory + '/' + filename\n",
    "        #add doc to vocab\n",
    "        add_to_vocab(file_path, vocab)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Now we pass all the file in training set to process_documents() function and create a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29401\n",
      "[('The', 5824), ('said', 5439), ('Mr', 2593), ('would', 1924), ('also', 1462), ('US', 1211), ('But', 1197), ('He', 1169), ('people', 1166), ('It', 1067), ('year', 1066), ('new', 1010), ('could', 977), ('one', 964), ('government', 909), ('years', 883), ('last', 816), ('In', 749), ('two', 749), ('first', 734), ('UK', 722), ('time', 721), ('told', 701), ('best', 697), ('We', 670), ('film', 656), ('Labour', 614), ('made', 578), ('election', 577), ('make', 566), ('BBC', 533), ('Blair', 523), ('get', 523), ('added', 507), ('number', 482), ('music', 481), ('next', 478), ('says', 476), ('three', 474), ('like', 466), ('take', 465), ('back', 457), ('say', 456), ('many', 451), ('public', 449), ('British', 432), ('set', 429), ('company', 428), ('way', 424), ('plans', 418)]\n"
     ]
    }
   ],
   "source": [
    "#define the vocab as counter\n",
    "vocab = Counter()\n",
    "#add all docs to Vocab\n",
    "process_documents('Train ready Dataset/train/neg', vocab)\n",
    "process_documents('Train ready Dataset/train/pos', vocab)\n",
    "#print the size of the vocab\n",
    "print(len(vocab))\n",
    "#print the top 50 most common words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>We can remove the tokens from the vocab which has low occurence as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17850\n"
     ]
    }
   ],
   "source": [
    "#keep tokens with a min occurence\n",
    "minimum_occurence = 2\n",
    "tokens = [tkn for tkn,count in vocab.items() if count >=minimum_occurence]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b> We can save the vocabulary into a .txt file which can later be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a tokens to a file\n",
    "def save_vocab(tokens, filename):\n",
    "    #convert tokens to single blob of text\n",
    "    data = '\\n'.join(tokens)\n",
    "    #open file in write mode\n",
    "    file = open(filename, 'w')\n",
    "    #write the text to a file\n",
    "    file.write(data)\n",
    "    #close file\n",
    "    file.close()  \n",
    "#save to to vocabulary file\n",
    "save_vocab(tokens,'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Train Embedding Layer</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Word embedding is any of a set of language modeling and feature learning techniques in natural language processing where words or phrases from the vocabulary are mapped to vectors of real numbers. Now we will learn a word embedding using keras while training a neural network on the classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Loading vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>We will load the our vocabulary file into a memory by using load_file() function written before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the vocabulary into memory\n",
    "vocab_file = 'vocab.txt'\n",
    "vocab = load_file(vocab_file)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Now we need to load all training data into memory.Before that, we need to clean them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>clean_text_file() function below converts the text into clean tokens by removing punctuations, and removing the tokens which are not in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the text into clean tokens\n",
    "def clean_text_file(doc, vocab):\n",
    "    #split the text into tokens by whitespace\n",
    "    tokens = doc.split()\n",
    "    #remove the punctuation from each token\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    #remove the token which are not in the vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>process_text_documents() function below loads all the documents and passes it to clean_text_file() function to clean it and  then append the tokens  to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the documents in a directory\n",
    "def process_text_documents(directory, vocab):\n",
    "    documents = list()\n",
    "    #walk through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        #create the full path of the file which is to be opened\n",
    "        file_path = directory + '/' + filename\n",
    "        #load the document\n",
    "        doc = load_file(file_path)\n",
    "        #clean the document\n",
    "        tokens = clean_text_file(doc, vocab)\n",
    "        #add to list\n",
    "        documents.append(tokens)\n",
    "    return documents                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Loading all the positive and negative documents in the training dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the training dataset\n",
    "positive_documents = process_text_documents('Train ready Dataset/train/pos', vocab)\n",
    "negative_documents = process_text_documents('Train ready Dataset/train/neg', vocab)\n",
    "train_docs = negative_documents + positive_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now, we will use Keras Tokenizer API.fit_on_texts() will create a vocabulary of all tokens in the training set and will develop a consistent mappping from words in the vocabulary to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "#fit the tokenizer on the training documents\n",
    "tokenizer.fit_on_texts(train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now,texts_to_sequence will encode each document in the training set into a seuence of unique integers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For this project, maximum length of document is set to 400.Hence we will pad and truncate every documents in the training set to a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sequences\n",
    "max_length = 400\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen = max_length, padding = 'post',truncating = 'post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Training labels for the documents are defined according to sentiment in training dataset. 0 is defined as label for documents in \"neg\" folder having negative sentiment.Similarly, for all documents in \"pos\" folder having positive sentiment are given label of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training labels\n",
    "import numpy as np\n",
    "ytrain = np.array([0 for _ in range(778)] +  [1 for _ in range(778)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Similarly, all documents in the test dataset are loaded and encoded into sequence of unique integers.After that, they are padded to maximum length of 400 and the test labels of a documents are passed in a similar way as that of training dataset(i.e 0 label for negative sentiment document and 1 for positive sentiment document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all test reviews\n",
    "positive_docs = process_text_documents('Train ready Dataset/test/pos', vocab)\n",
    "negative_docs = process_text_documents('Train ready Dataset/test/neg', vocab)\n",
    "test_docs = negative_docs + positive_docs\n",
    "#sequence encode (Note: we do not do tokenizer.fit_on_texts on test data otherwise it will change index of words.)\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "#pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen = max_length, padding = 'post',truncating = 'post')\n",
    "#define test labels\n",
    "ytest = np.array( [0 for _ in range(40)] + [1 for _ in range(40)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now we define the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define vocabulary  size(largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Since, we are going to use softmax classifier later, so all the train and test label must be converted into one hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding the y labels\n",
    "ytrain = tf.keras.utils.to_categorical(ytrain, 2)\n",
    "ytest = tf.keras.utils.to_categorical(ytest, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now model is built. We use Stacked Bidirectional LSTM(BiLSTM) with dropout of 20%. Since it is multi-class classification, so softmax classifier is used.The summary of a model is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 100)          1598900   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 400, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 400, 128)          84480     \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 1,782,454\n",
      "Trainable params: 1,782,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length = max_length))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(LSTM(64, dropout =0.2,return_sequences= True)))\n",
    "model.add(Bidirectional(LSTM(64, dropout =0.2)))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now, we will compile the model. Since we have used softmax classifer, we will use categorical_crossentropy as a loss function. Similarly, Adam optimizer was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the network\n",
    "model.compile(loss= 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Fitting the training dataset into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49/49 - 39s - loss: 0.6193 - accuracy: 0.6350\n",
      "Epoch 2/10\n",
      "49/49 - 39s - loss: 0.2321 - accuracy: 0.9094\n",
      "Epoch 3/10\n",
      "49/49 - 40s - loss: 0.0522 - accuracy: 0.9833\n",
      "Epoch 4/10\n",
      "49/49 - 38s - loss: 0.0182 - accuracy: 0.9949\n",
      "Epoch 5/10\n",
      "49/49 - 40s - loss: 0.0148 - accuracy: 0.9968\n",
      "Epoch 6/10\n",
      "49/49 - 39s - loss: 0.0152 - accuracy: 0.9968\n",
      "Epoch 7/10\n",
      "49/49 - 41s - loss: 0.0155 - accuracy: 0.9981\n",
      "Epoch 8/10\n",
      "49/49 - 42s - loss: 0.0213 - accuracy: 0.9955\n",
      "Epoch 9/10\n",
      "49/49 - 36s - loss: 0.0186 - accuracy: 0.9968\n",
      "Epoch 10/10\n",
      "49/49 - 36s - loss: 0.0134 - accuracy: 0.9981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa23c0c2dd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the network\n",
    "model.fit(Xtrain, ytrain, epochs = 10, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now we will evaluate the performance of our model on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 78.750002\n"
     ]
    }
   ],
   "source": [
    "#evaluate\n",
    "loss,acc =model.evaluate(Xtest, ytest, verbose = 0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The model has a test accuracy of 78.75% which is  decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
