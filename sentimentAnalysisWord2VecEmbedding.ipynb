{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>News Sentiment Analyzer using Bidirectional LSTM(BiLSTM) and Word2Vec Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import os\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional, SpatialDropout1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We have a dataset named \"Train ready Dataset\" which has test and train folders. Inside both \"test\" and \"train\" folders there are \"neg\" and \"pos\" subfolder. There are 778 .txt files inside both \"neg\" and \"pos\" subfolder of \"train\" set, while there are 40 .txt files inside both \"neg\" and \"pos\" subfolder of \"test\" set. The text file are news excerpt from BBC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Loading and Cleaning .txt files</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>load_file() function below loads the file,opens the file in read only mode and returns text of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load doc into the memory\n",
    "def load_file(filename):\n",
    "    #opening the file in read only mode\n",
    "    file = open(filename, 'r')\n",
    "    #read all text\n",
    "    text = file.read()\n",
    "    #close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>clean_file() function below converts the text into clean tokens by removing punctuations,stop words and short words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the text into clean tokens\n",
    "def clean_file(text):\n",
    "    #split the text into tokens by whitespace\n",
    "    tokens = text.split()\n",
    "    #remove punctuation from each token\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    #remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    #remove the stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    #remove the short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Example of loading and cleaning a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Water', 'firm', 'Suez', 'Argentina', 'row', 'conflict', 'Argentine', 'State', 'water', 'firm', 'Aguas', 'Argentinas', 'controlled', 'Frances', 'Suez', 'casting', 'doubt', 'firms', 'future', 'The', 'firm', 'serves', 'province', 'Buenos', 'Aires', 'wants', 'tariff', 'rise', 'fund', 'watersupply', 'improvements', 'The', 'government', 'rejected', 'rise', 'wants', 'Aguas', 'Argentinas', 'make', 'annual', 'investment', 'pesos', 'improvements', 'Planning', 'Minister', 'Julio', 'De', 'Vido', 'offered', 'State', 'help', 'free', 'Mr', 'De', 'Vido', 'said', 'Argentine', 'state', 'would', 'make', 'contribution', 'form', 'subsidy', 'He', 'said', 'contribution', 'could', 'made', 'return', 'seat', 'companys', 'board', 'He', 'added', 'government', 'discussions', 'Aguas', 'Argentinas', 'role', 'might', 'take', 'event', 'State', 'contribution', 'agreed', 'However', 'Aguas', 'Argentinas', 'told', 'Argentine', 'newspaper', 'Clarin', 'would', 'accept', 'change', 'legal', 'structure', 'practice', 'rules', 'State', 'participation', 'board', 'The', 'Planning', 'Minister', 'didnt', 'rule', 'possibility', 'cancelling', 'Aguas', 'Argentinas', 'water', 'concession', 'Yet', 'added', 'didnt', 'like', 'futurology', 'But', 'last', 'week', 'Argentine', 'Economic', 'Minister', 'Roberto', 'Lavagna', 'told', 'French', 'media', 'Paris', 'government', 'considering', 'allowing', 'increase', 'tariffs', 'possibility', 'State', 'contribution', 'Aguas', 'Argentinas', 'infrastructure', 'investments', 'Speaking', 'Buenos', 'Aires', 'Mr', 'De', 'Vido', 'later', 'denied', 'possibility', 'tariff', 'increase', 'insisted', 'annual', 'investment', 'water', 'infrastructure', 'centre', 'discussions', 'He', 'added', 'coming', 'weeks', 'future', 'Aguas', 'Argentinas', 'would', 'decided', 'Suez', 'owns', 'Aguas', 'Argentinas', 'Spains', 'Aguas', 'de', 'Barcelona', 'second', 'biggest', 'shareholder', 'Recently', 'Suez', 'lost', 'water', 'concession', 'Bolivia', 'mass', 'protests', 'city', 'El', 'Alto', 'poorest', 'country', 'citizens', 'complaining', 'unfair', 'water', 'charges', 'This', 'forced', 'government', 'cancel', 'contract', 'In', 'Argentina', 'Suezs', 'subsidiary', 'fined', 'cutting', 'supply', 'water', 'recent', 'heat', 'wave', 'allegedly', 'failing', 'keep', 'investment', 'meet', 'demand', 'water', 'maintained', 'tense', 'relationship', 'Argentine', 'government', 'During', 'last', 'financial', 'crisis', 'Argentina', 'firm', 'sued', 'state', 'alleging', 'converting', 'tariffs', 'US', 'dollars', 'pesos', 'freezing', 'devaluation', 'affected', 'company', 'made', 'difficult', 'meet', 'contractual', 'obligations', 'When', 'President', 'Nestor', 'Kirchner', 'Argentina', 'arrived', 'power', 'began', 'negotiate', 'solution', 'disagreements', 'international', 'utilities', 'operating', 'Argentina', 'But', 'rejected', 'tariff', 'increases', 'alleging', 'impoverish', 'citizens', 'He', 'also', 'asked', 'investments', 'meet', 'growing', 'demand', 'water', 'On', 'May', 'Aguas', 'Argentinas', 'government', 'signed', 'agreement', 'renegotiate', 'Buenos', 'Aires', 'waterconcession', 'contract', 'The', 'firm', 'agreed', 'invest', 'pesos', 'The', 'issue', 'attracted', 'European', 'interest', 'Last', 'week', 'Paris', 'President', 'Kirchner', 'discussed', 'problem', 'issues', 'French', 'president', 'Jacques', 'Chirac', 'The', 'Argentine', 'government', 'also', 'pressure', 'European', 'Union', 'countries', 'International', 'Monetary', 'Fund', 'IMF', 'raise', 'utilities', 'tariffs', 'utilities', 'operating', 'Argentina', 'European']\n"
     ]
    }
   ],
   "source": [
    "#Load and clean the document\n",
    "filename = \"Train ready Dataset/train/neg/93.txt\"\n",
    "text = load_file(filename)\n",
    "tokens = clean_file(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Define a Vocabulary</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>add_to_vocab() function performs the task of loading the document, cleaning the document and returning the tokens which is later added to vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc and add to vocab\n",
    "def add_to_vocab(filename, vocab):\n",
    "    #load  document\n",
    "    doc = load_file(filename)\n",
    "    #clean document\n",
    "    tokens = clean_file(doc)\n",
    "    #add tokens to the vocabulary\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>process_documents() function will load all the files from the directory and pass the file to add_to_vocab() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all docs in a vocabulary\n",
    "def process_documents(directory, vocab):\n",
    "    #walk through all files in the folder\n",
    "    for filename in os.listdir(directory):\n",
    "        #create a full path of the file to open\n",
    "        file_path = directory + '/' + filename\n",
    "        #add doc to vocab\n",
    "        add_to_vocab(file_path, vocab) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Now we pass all the file in training set to process_documents() function and create a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29401\n",
      "[('The', 5824), ('said', 5439), ('Mr', 2593), ('would', 1924), ('also', 1462), ('US', 1211), ('But', 1197), ('He', 1169), ('people', 1166), ('It', 1067), ('year', 1066), ('new', 1010), ('could', 977), ('one', 964), ('government', 909), ('years', 883), ('last', 816), ('In', 749), ('two', 749), ('first', 734), ('UK', 722), ('time', 721), ('told', 701), ('best', 697), ('We', 670), ('film', 656), ('Labour', 614), ('made', 578), ('election', 577), ('make', 566), ('BBC', 533), ('Blair', 523), ('get', 523), ('added', 507), ('number', 482), ('music', 481), ('next', 478), ('says', 476), ('three', 474), ('like', 466), ('take', 465), ('back', 457), ('say', 456), ('many', 451), ('public', 449), ('British', 432), ('set', 429), ('company', 428), ('way', 424), ('plans', 418)]\n"
     ]
    }
   ],
   "source": [
    "#define the vocab as counter\n",
    "vocab = Counter()\n",
    "#add all docs to Vocab\n",
    "process_documents('Train ready Dataset/train/neg', vocab)\n",
    "process_documents('Train ready Dataset/train/pos', vocab)\n",
    "#print the size of the vocab\n",
    "print(len(vocab))\n",
    "#print the top 50 most common words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>We can remove the tokens from the vocab which has low occurence as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17850\n"
     ]
    }
   ],
   "source": [
    "#keep tokens with a min occurence\n",
    "minimum_occurence = 2\n",
    "tokens = [tkn for tkn,count in vocab.items() if count >=minimum_occurence]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b> We can save the vocabulary into a .txt file which can later be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a tokens to a file\n",
    "def save_vocab(tokens, filename):\n",
    "    #convert tokens to single blob of text\n",
    "    data = '\\n'.join(tokens)\n",
    "    #open file in write mode\n",
    "    file = open(filename, 'w')\n",
    "    #write the text to a file\n",
    "    file.write(data)\n",
    "    #close file\n",
    "    file.close()  \n",
    "#save to to vocabulary file\n",
    "save_vocab(tokens,'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Train Word2vec Embedding </b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Word embedding is any of a set of language modeling and feature learning techniques in natural language processing where words or phrases from the vocabulary are mapped to vectors of real numbers. Now we will learn a Word2Vec embedding while training a neural network on the classification problem.The word2vec algorithm is an approach to learning a word embedding from a text corpus in a standalone way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Load the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_file(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now, we define a doc_to_clean_lines() function to clean a loaded document line by line and return a list of the cleaned lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn a doc into clean tokens\n",
    "def doc_to_clean_lines(doc, vocab):\n",
    "    clean_lines = list()\n",
    "    lines = doc.splitlines()\n",
    "    for line in lines:\n",
    "        #split into tokens by whitespace\n",
    "        tokens = line.split()\n",
    "        #remove punctuation from each token\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        #remove the token which are not in the vocab\n",
    "        tokens = [ w for w in tokens if w in vocab]\n",
    "        clean_lines.append(tokens)\n",
    "    return clean_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Then we use process_doc_lines() function load and clean all of the documents in a folder and return a list of all document lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all docs in a vocabulary\n",
    "def process_docs_lines(directory, vocab):\n",
    "    lines = list()\n",
    "    #walk through all files in the folder\n",
    "    for filename in os.listdir(directory):\n",
    "        #create a full path of the file to open\n",
    "        file_path = directory + '/' + filename\n",
    "        #load and clean doc\n",
    "        doc = load_file(file_path)\n",
    "        doc_lines = doc_to_clean_lines(doc, vocab)\n",
    "        #add lines to list\n",
    "        lines+= doc_lines\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The results from process_docs_lines() function will be the training data for the word2vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now, we can load all the training data and convert it into  long  list of sentences ready for fitting the word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training sentences: 16222\n"
     ]
    }
   ],
   "source": [
    "#load training data\n",
    "positive_docs = process_docs_lines('Train ready Dataset/train/pos', vocab)\n",
    "negative_docs = process_docs_lines('Train ready Dataset/train/neg', vocab)\n",
    "sentences = negative_docs + positive_docs\n",
    "print('Total training sentences: %d' % len(sentences)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We will use word2vec algorithm provided by python gensim library.We define embedding vector space = 100.Similarly windows size of 5 defines the number of neighbouring words to look to learn embedding. min_count = 1 is the minimum times the word must occur to be considered in a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 17850\n"
     ]
    }
   ],
   "source": [
    "#train word2vec model\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences, size=100, window=5, workers=8, min_count=1)\n",
    "# summarize vocabulary size in model\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))\n",
    "# save model in ASCII (word2vec) format\n",
    "filename = 'embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Use Pre-trained Embedding </b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>We can use pretrained word embedding developed earlier.Since word embedding was saved in 'word2vec' format, we should load word embedding as the directory of words to vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>load_embedding() function loads the embedding file and returns a directory of words mapped to vectors in Numpy format.Since embedding file has header in first line, we will skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load embedding as a dictionary\n",
    "def load_embedding(filename):\n",
    "    #load embedding into memory,skip first line\n",
    "    file = open(filename, 'r')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "    #create a map of words to vectors\n",
    "    embedding  = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        #key is string word, valaue is numpy array for vector\n",
    "        embedding[parts[0]] = np.asarray(parts[1:], dtype = 'float32')\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now we will create the embedding layer from a loaded embedding.This will be the 1st layer of our model.The get_weight_matrix() function takes the loaded embedding and the tokenizer.word_index vocabulary as arguments and returns a matrix with the word vectors in the correct locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    #total vocabulary sizw plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    #define weight matrix  dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = embedding.get(word)\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Now we need to load all training data into memory.Before that, we need to clean them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>clean_text_file() function below converts the text into clean tokens by removing punctuations, and removing the tokens which are not in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the text into clean tokens\n",
    "def clean_text_file(doc, vocab):\n",
    "    #split the text into tokens by whitespace\n",
    "    tokens = doc.split()\n",
    "    #remove the punctuation from each token\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    #remove the token which are not in the vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>process_text_documents() function below loads all the documents and passes it to clean_text_file() function to clean it and  then append the tokens  to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the documents in a directory\n",
    "def process_text_documents(directory, vocab):\n",
    "    documents = list()\n",
    "    #walk through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        #create the full path of the file which is to be opened\n",
    "        file_path = directory + '/' + filename\n",
    "        #load the document\n",
    "        doc = load_file(file_path)\n",
    "        #clean the document\n",
    "        tokens = clean_text_file(doc, vocab)\n",
    "        #add to list\n",
    "        documents.append(tokens)\n",
    "    return documents      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Loading all the positive and negative documents in the training dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the training dataset\n",
    "positive_documents = process_text_documents('Train ready Dataset/train/pos', vocab)\n",
    "negative_documents = process_text_documents('Train ready Dataset/train/neg', vocab)\n",
    "train_docs = negative_documents + positive_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now, we will use Keras Tokenizer API.fit_on_texts() will create a vocabulary of all tokens in the training set and will develop a consistent mappping from words in the vocabulary to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "#fit the tokenizer on the training documents\n",
    "tokenizer.fit_on_texts(train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now,texts_to_sequence will encode each document in the training set into a seuence of unique integers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For this project, maximum length of document is set to 400.Hence we will pad and truncate every documents in the training set to a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sequences\n",
    "max_length = 400\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen = max_length, padding = 'post',truncating = 'post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Training labels for the documents are defined according to sentiment in training dataset. 0 is defined as label for documents in \"neg\" folder having negative sentiment.Similarly, for all documents in \"pos\" folder having positive sentiment are given label of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training labels\n",
    "import numpy as np\n",
    "ytrain = np.array([0 for _ in range(778)] +  [1 for _ in range(778)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Similarly, all documents in the test dataset are loaded and encoded into sequence of unique integers.After that, they are padded to maximum length of 400 and the test labels of a documents are passed in a similar way as that of training dataset(i.e 0 label for negative sentiment document and 1 for positive sentiment document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all test reviews\n",
    "positive_docs = process_text_documents('Train ready Dataset/test/pos', vocab)\n",
    "negative_docs = process_text_documents('Train ready Dataset/test/neg', vocab)\n",
    "test_docs = negative_docs + positive_docs\n",
    "#sequence encode (Note: we do not do tokenizer.fit_on_texts on test data otherwise it will change index of words.)\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "#pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen = max_length, padding = 'post',truncating = 'post')\n",
    "#define test labels\n",
    "ytest = np.array( [0 for _ in range(40)] + [1 for _ in range(40)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now we define the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define vocabulary  size(largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Since, we are going to use softmax classifier later, so all the train and test label must be converted into one hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding the y labels\n",
    "ytrain = tf.keras.utils.to_categorical(ytrain, 2)\n",
    "ytest = tf.keras.utils.to_categorical(ytest, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load embedding from file\n",
    "raw_embedding = load_embedding('embedding_word2vec.txt')\n",
    "#get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "#create a embedding layer\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors],input_length = max_length, trainable = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now model is built.We use embedding layer developed from pretrained word2vec model. Similarly, we use Stacked Bidirectional LSTM(BiLSTM) with dropout of 20%. Since it is multi-class classification, so softmax classifier is used.The summary of a model is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 100)          1598900   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 400, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 400, 128)          84480     \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 1,782,454\n",
      "Trainable params: 183,554\n",
      "Non-trainable params: 1,598,900\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(LSTM(64, dropout =0.2,return_sequences= True)))\n",
    "model.add(Bidirectional(LSTM(64, dropout =0.2)))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now, we will compile the model. Since we have used softmax classifer, we will use categorical_crossentropy as a loss function. Similarly, Adam optimizer was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the network\n",
    "model.compile(loss= 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Fitting the training dataset into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60/60 - 3s - loss: 0.7177 - accuracy: 0.5363\n",
      "Epoch 2/10\n",
      "60/60 - 3s - loss: 0.6711 - accuracy: 0.5958\n",
      "Epoch 3/10\n",
      "60/60 - 3s - loss: 0.6032 - accuracy: 0.6795\n",
      "Epoch 4/10\n",
      "60/60 - 3s - loss: 0.5003 - accuracy: 0.7763\n",
      "Epoch 5/10\n",
      "60/60 - 3s - loss: 0.4006 - accuracy: 0.8347\n",
      "Epoch 6/10\n",
      "60/60 - 3s - loss: 0.2896 - accuracy: 0.9068\n",
      "Epoch 7/10\n",
      "60/60 - 4s - loss: 0.2000 - accuracy: 0.9479\n",
      "Epoch 8/10\n",
      "60/60 - 5s - loss: 0.1268 - accuracy: 0.9795\n",
      "Epoch 9/10\n",
      "60/60 - 5s - loss: 0.0861 - accuracy: 0.9921\n",
      "Epoch 10/10\n",
      "60/60 - 4s - loss: 0.0496 - accuracy: 0.9989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd76c458050>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now we will evaluate the performance of our model on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 60.000002\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The model has a test accuracy of 60% which is much less than what we got with keras Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
